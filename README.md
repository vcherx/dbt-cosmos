# dbt-cosmos
Quickstart from https://quickstarts.snowflake.com/guide/data_engineering_with_apache_airflow/ 

## General Instructions
### Required packages:
- [Astro Cli](https://docs.astronomer.io/astro/cli/overview)
- [Dbt Core](https://docs.getdbt.com/docs/core/installation-overview)
- [Dbt adapter for Snowflake](https://docs.getdbt.com/docs/core/connect-data-platform/snowflake-setup)
- [Docker Desktop](https://docs.docker.com/desktop/)
- Python and Pip
- [Snowflake Account](https://signup.snowflake.com/)

## Workarounds :D
- Create the WareHouse, Role and User first in Snowflake, by following the instructions on part 3 setting up our dbt Project. 
- Go back to part 2 - Set Up environment and configure the dbt project. 
- Part 4: Create the csv in the seeds folder. Data is deprecated.

### References
- [To understand Macros and Ref](https://medium.com/@rasiksuhail/exploring-about-macros-ref-in-dbt-11bce7448c92) 
- [Seeds](https://www.youtube.com/watch?v=CWetaAaEdKw&ab_channel=KahanDataSolutions)
